{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2f267-38c0-45d2-8d40-c2251624ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5492160-b01a-4b5b-820a-37b9f498e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\SAKET\\\\OneDrive\\\\Desktop\\\\AIML\\\\DL-Models\\\\Flood Detection\\\\Flood_dataset\"  \n",
    "\n",
    "image_path = os.path.join(dataset_path, \"Image\")\n",
    "mask_path = os.path.join(dataset_path, \"Mask\")\n",
    "metadata_path = os.path.join(dataset_path, \"metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079dd2f7-e7c4-475e-acee-46ce2233b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(metadata_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577ccad-2acd-4782-9a9e-2bd40e51e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display an image and its corresponding mask\n",
    "def visualize_image_mask(index):\n",
    "    img_name = df.iloc[index, 0]  # First column contains image filenames\n",
    "    mask_name = df.iloc[index, 1]  # Second column contains mask filenames\n",
    "\n",
    "    img_path = os.path.join(image_path, img_name)\n",
    "    mask_path_full = os.path.join(mask_path, mask_name)\n",
    "\n",
    "    # Debugging: Print paths to check if they are correct\n",
    "    print(f\"Image path: {img_path}\")\n",
    "    print(f\"Mask path: {mask_path_full}\")\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    # Check if the image was loaded correctly\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image at {img_path}\")\n",
    "        return\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "    # Load mask\n",
    "    mask = cv2.imread(mask_path_full, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if mask is None:\n",
    "        print(f\"Error: Could not load mask at {mask_path_full}\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Flood Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=\"gray\")\n",
    "    plt.title(\"Segmentation Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Try displaying a sample image\n",
    "visualize_image_mask(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6412cc-7d19-4f29-ab16-df924f4a7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def check_image_loading(image_path):\n",
    "    print(f\"Checking: {image_path}\")\n",
    "\n",
    "    # Check with OpenCV\n",
    "    img_cv = cv2.imread(image_path)\n",
    "    if img_cv is None:\n",
    "        print(\"⚠️ OpenCV cannot read the image!\")\n",
    "\n",
    "    # Check with PIL\n",
    "    try:\n",
    "        img_pil = Image.open(image_path)\n",
    "        print(\"✅ PIL successfully loaded the image!\")\n",
    "        img_pil.show()  # Opens the image\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ PIL error: {e}\")\n",
    "\n",
    "img_path = r\"C:\\Users\\SAKET\\OneDrive\\Desktop\\AIML\\DL-Models\\Flood Detection\\Flood_dataset\\Image\\0.jpg\"\n",
    "check_image_loading(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e88e2b-b038-4a16-84df-13acb014ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Define the image path\n",
    "img_path = r\"C:\\Users\\SAKET\\OneDrive\\Desktop\\AIML\\DL-Models\\Flood Detection\\Flood_dataset\\Image\\0.jpg\"\n",
    "\n",
    "# Open the image with PIL\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Convert the image to standard RGB format\n",
    "img = img.convert(\"RGB\")\n",
    "\n",
    "# Save the image again as a proper JPEG\n",
    "img.save(img_path, format=\"JPEG\")\n",
    "\n",
    "print(\"✅ Image successfully converted and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d7122-1f0e-48db-a935-cd6be019d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img_path = r\"C:\\Users\\SAKET\\OneDrive\\Desktop\\AIML\\DL-Models\\Flood Detection\\Flood_dataset\\Image\\0.jpg\"\n",
    "\n",
    "# Try loading the image again\n",
    "img = cv2.imread(img_path)\n",
    "if img is None:\n",
    "    print(\"⚠️ OpenCV STILL cannot read the image! Try moving the dataset out of OneDrive.\")\n",
    "else:\n",
    "    print(\"✅ OpenCV successfully loaded the image!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c7c26-6b69-438b-92e4-ede9763e3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "image_folder = r\"C:\\Users\\SAKET\\OneDrive\\Desktop\\AIML\\DL-Models\\Flood Detection\\Flood_dataset\\Image\"\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    img_path = os.path.join(image_folder, filename)\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img.save(img_path, format=\"JPEG\")  # Overwrite in JPEG format\n",
    "        print(f\"✅ Fixed: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not fix {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeece4ca-83ab-4039-a631-d0936e1be3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define dataset paths\n",
    "image_path = r\"C:\\Users\\SAKET\\OneDrive\\Desktop\\AIML\\DL-Models\\Flood Detection\\Flood_dataset\\Image\"\n",
    "mask_path = r\"C:\\Users\\SAKET\\OneDrive\\Desktop\\AIML\\DL-Models\\Flood Detection\\Flood_dataset\\Mask\"\n",
    "\n",
    "# Load the metadata CSV\n",
    "csv_path = r\"C:\\Users\\SAKET\\OneDrive\\Desktop\\AIML\\DL-Models\\Flood Detection\\Flood_dataset\\metadata.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Function to visualize an image and its mask\n",
    "def visualize_image_mask(index):\n",
    "    img_name = df.iloc[index, 0]  # First column contains image filenames\n",
    "    mask_name = df.iloc[index, 1]  # Second column contains mask filenames\n",
    "\n",
    "    img = cv2.imread(os.path.join(image_path, img_name))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "    mask = cv2.imread(os.path.join(mask_path, mask_name), cv2.IMREAD_GRAYSCALE)  # Load mask in grayscale\n",
    "\n",
    "    # Display both side by side\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Flood Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=\"gray\")\n",
    "    plt.title(\"Segmentation Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Test by visualizing the first image-mask pair\n",
    "visualize_image_mask(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f061d3-e999-4b3e-9d33-6f02e6f9527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define image dimensions (adjust if needed)\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "\n",
    "def preprocess_image(image_path, mask_path):\n",
    "    # Load and resize the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # Resize\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Load and resize the mask\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "    mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT))  # Resize\n",
    "    mask = mask / 255.0  # Normalize to [0, 1]\n",
    "    mask = np.expand_dims(mask, axis=-1)  # Add channel dimension\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "# Test with a sample image\n",
    "sample_image_path = os.path.join(image_path, df.iloc[0, 0])\n",
    "sample_mask_path = os.path.join(mask_path, df.iloc[0, 1])\n",
    "\n",
    "sample_img, sample_mask = preprocess_image(sample_image_path, sample_mask_path)\n",
    "\n",
    "# Display preprocessed image & mask\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_img)\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sample_mask.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"Preprocessed Mask\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69033222-1f9a-49ae-87b0-668739003512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image_path, mask_path):\n",
    "    # Load image with OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # Resize\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Load mask with PIL (since OpenCV sometimes fails)\n",
    "    mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "    mask = mask.resize((IMG_WIDTH, IMG_HEIGHT))  # Resize\n",
    "    mask = np.array(mask) / 255.0  # Convert to NumPy and normalize\n",
    "    mask = np.expand_dims(mask, axis=-1)  # Add channel dimension\n",
    "\n",
    "    return img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f47d51-3ca3-4297-acc5-5e030cd25774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "# Paths\n",
    "image_dir = \"C:\\\\Users\\\\SAKET\\\\OneDrive\\\\Desktop\\\\AIML\\\\DL-Models\\\\Flood Detection\\\\Flood_dataset\\\\Image\"\n",
    "mask_dir = \"C:\\\\Users\\\\SAKET\\\\OneDrive\\\\Desktop\\\\AIML\\\\DL-Models\\\\Flood Detection\\\\Flood_dataset\\\\Mask\"\n",
    "\n",
    "# Read the metadata CSV file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\SAKET\\\\OneDrive\\\\Desktop\\\\AIML\\\\DL-Models\\\\Flood Detection\\\\Flood_dataset\\\\metadata.csv\")\n",
    "\n",
    "# Lists to store images and masks\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "# Load all images and masks\n",
    "for i in range(len(df)):\n",
    "    img_path = os.path.join(image_dir, df.iloc[i, 0])  # Image filename\n",
    "    mask_path = os.path.join(mask_dir, df.iloc[i, 1])  # Mask filename\n",
    "\n",
    "    # Load image\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    img = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Load mask\n",
    "    mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "    mask = mask.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    mask = np.array(mask) / 255.0  # Normalize to [0, 1]\n",
    "    mask = np.expand_dims(mask, axis=-1)  # Add channel dimension\n",
    "\n",
    "    images.append(img)\n",
    "    masks.append(mask)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "images = np.array(images)\n",
    "masks = np.array(masks)\n",
    "\n",
    "print(f\"Dataset Loaded: {images.shape[0]} images, {masks.shape[0]} masks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1800bf1-2abc-48d9-9cba-d5fb21769f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Data: {X_train.shape[0]} images, Validation Data: {X_val.shape[0]} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3ecf1-d446-4036-881e-107188a48bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define augmentation transformations\n",
    "data_gen_args = dict(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "# Create ImageDataGenerator objects\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Generate augmented images and masks\n",
    "image_generator = image_datagen.flow(X_train, batch_size=8, seed=42)\n",
    "mask_generator = mask_datagen.flow(Y_train, batch_size=8, seed=42)\n",
    "\n",
    "# Combine generators\n",
    "train_generator = zip(image_generator, mask_generator)\n",
    "\n",
    "# Check one augmented sample\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "augmented_images, augmented_masks = next(train_generator)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(augmented_images[0])  \n",
    "axes[0].set_title(\"Augmented Image\")\n",
    "\n",
    "axes[1].imshow(augmented_masks[0].squeeze(), cmap=\"gray\")  \n",
    "axes[1].set_title(\"Augmented Mask\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0b8d7-72d5-41a8-b6ba-a21b974c7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ab9c6-dc0a-4b1d-bbc6-d3819750e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder (Downsampling)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # Decoder (Upsampling)\n",
    "    u6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271e787-7df6-428d-b17b-807271c3e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize U-Net model\n",
    "model = build_unet_model()\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e167e-86fe-4d3e-b9e0-4565388b0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b7d8c-4256-4fd5-816c-e578ad42fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=BinaryCrossentropy(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c1847-8963-4810-80b1-c69c3df5ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and epochs\n",
    "batch_size = 8\n",
    "epochs = 25\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(image_generator, \n",
    "                    steps_per_epoch=len(X_train) // batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=mask_generator, \n",
    "                    validation_steps=len(Y_train) // batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ded030-d2f4-4a37-8293-578d6e93ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {np.shape(X_train)}\")\n",
    "print(f\"Y_train shape: {np.shape(Y_train)}\")\n",
    "\n",
    "# Check if any None values exist\n",
    "print(\"Any None in X_train?\", any(x is None for x in X_train))\n",
    "print(\"Any None in Y_train?\", any(y is None for y in Y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885837a-192f-458f-aa3c-6c4b2df25917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(image_generator, \n",
    "                    steps_per_epoch=max(1, len(X_train) // batch_size), \n",
    "                    epochs=epochs, \n",
    "                    validation_data=mask_generator, \n",
    "                    validation_steps=max(1, len(Y_train) // batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c5158-55eb-4100-946f-f52ecc7b10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train type: {type(X_train)}, shape: {np.shape(X_train)}\")\n",
    "print(f\"Y_train type: {type(Y_train)}, shape: {np.shape(Y_train)}\")\n",
    "\n",
    "print(\"Any None in X_train?\", any(x is None for x in X_train))\n",
    "print(\"Any None in Y_train?\", any(y is None for y in Y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b26ed-67fa-4199-b684-128e2ecb9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(image_generator)\n",
    "print(f\"Image batch shape: {x_batch.shape}\")\n",
    "print(f\"Mask batch shape: {y_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ca43a-a15e-4418-bbf6-c853d0fb78c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(image_generator)\n",
    "print(type(batch))\n",
    "print(len(batch))  # Should be 2 (images, masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec061f-36a2-4378-85cc-08153b01b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Data augmentation parameters\n",
    "data_gen_args = dict(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Create separate generators for images and masks\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "batch_size = 8  # Ensure batch size is defined\n",
    "\n",
    "# Ensure X_train and Y_train are provided correctly\n",
    "image_generator = image_datagen.flow(X_train, Y_train, batch_size=batch_size, seed=42)\n",
    "\n",
    "# Get a batch from the generator\n",
    "x_batch, y_batch = next(image_generator)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Image batch shape: {x_batch.shape}\")  # Expected: (batch_size, 256, 256, 3)\n",
    "print(f\"Mask batch shape: {y_batch.shape}\")  # Expected: (batch_size, 256, 256, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6cd55-9518-4c35-96be-238e3e704520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define training parameters\n",
    "# epochs = 25\n",
    "# batch_size = 8  # Ensure this matches the generator\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     image_generator, \n",
    "#     steps_per_epoch=max(1, len(X_train) // batch_size), \n",
    "#     epochs=epochs, \n",
    "#     validation_data=(X_val, Y_val),  # Directly using validation data\n",
    "#     validation_steps=max(1, len(Y_val) // batch_size)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080aaaf-ae58-46ed-ad91-23a199b88317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Convert generators to TensorFlow datasets\n",
    "# train_dataset = tf.data.Dataset.from_generator(\n",
    "#     lambda: image_generator,\n",
    "#     output_signature=(\n",
    "#         tf.TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32),\n",
    "#         tf.TensorSpec(shape=(None, 256, 256, 1), dtype=tf.float32)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Prefetch for performance\n",
    "# train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192b0a1-75a4-4482-a76d-32669f89ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model with dataset\n",
    "# history = model.fit(\n",
    "#     train_dataset,  # Now using TensorFlow Dataset\n",
    "#     steps_per_epoch=len(X_train) // batch_size, \n",
    "#     epochs=25, \n",
    "#     validation_data=(X_val, Y_val), \n",
    "#     validation_steps=max(1, len(Y_val) // batch_size)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6063bd1-dc6d-4aba-9dc4-cce031047508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_batch, y_batch = next(iter(image_generator))\n",
    "# print(f\"Image batch shape: {x_batch.shape}\")\n",
    "# print(f\"Mask batch shape: {y_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5dfef-5ba1-4baf-a2f8-27ac0b0ad894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = tf.data.Dataset.from_generator(\n",
    "#     lambda: iter(image_generator),\n",
    "#     output_signature=(\n",
    "#         tf.TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32),\n",
    "#         tf.TensorSpec(shape=(None, 256, 256, 1), dtype=tf.float32)\n",
    "#     )\n",
    "# ).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc29e2-5b68-405e-b347-fb53e71b9a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(X_val), X_val.shape)\n",
    "# print(type(Y_val), Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea0804-c13e-4b36-bd6d-e55a99dce1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     train_dataset,\n",
    "#     steps_per_epoch=len(X_train) // batch_size,\n",
    "#     epochs=25,\n",
    "#     validation_data=(X_val, Y_val),\n",
    "#     validation_steps=max(1, len(Y_val) // batch_size)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8797c-daf0-4b41-9a78-8eb38bb784f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(train_dataset))\n",
    "# print(next(iter(train_dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bc0b7-c1b7-48c6-a9ef-75c0371a8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# batch_size = 8  # Ensure this matches previous values\n",
    "\n",
    "# # Convert NumPy arrays to TensorFlow datasets\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "# train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
    "# val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f885e5-d6db-40f9-8b2a-ea5adf5cc7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     train_dataset,\n",
    "#     epochs=25,\n",
    "#     validation_data=val_dataset\n",
    "# )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e0abde8-3b06-476e-929e-b6db54b527bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_val = to_categorical(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34a2d724-c8ec-45bd-8b99-2c0a4b395c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "Y_train = np.array(Y_train, dtype=np.float32)\n",
    "X_val = np.array(X_val, dtype=np.float32)\n",
    "Y_val = np.array(Y_val, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17dd1f29-2aaa-46f6-8b62-90726ec34ee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attr 'Toutput_types' of 'OptionalFromValue' Op passed list of length 0 less than minimum 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#metrics=[\"accuracy\"]\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:131\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOptional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     empty_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mOptional\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Attr 'Toutput_types' of 'OptionalFromValue' Op passed list of length 0 less than minimum 1."
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 25\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    #metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85c6aac0-8885-4c42-940b-cf3d8865f5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232, 256, 256, 3) (232, 256, 256, 1)\n",
      "(58, 256, 256, 3) (58, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63b05642-fa5a-4ae4-b37c-047a233942c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f41586-2425-4ed0-9f74-f6b40cbcc52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (spacy)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
